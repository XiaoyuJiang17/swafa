{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "folder_path = \"/home/v1xjian2/BDL/Bayesian_DL/datasets/aptos2019-blindness-detection/train_images\" \n",
    "\n",
    "files = os.listdir(folder_path)\n",
    "\n",
    "print(\"Num of files under this folder is: %d \" % len(files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aptos2019_train_df = pd.read_csv('/home/v1xjian2/BDL/Bayesian_DL/datasets/aptos2019-blindness-detection/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aptos2019_train_df['diagnosis'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binarize targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aptos2019_train_df['target'] = (aptos2019_train_df['diagnosis'] >=2)*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aptos2019_train_df['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aptos2019_train_df.to_csv('./binarised_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "target_0 = aptos2019_train_df[aptos2019_train_df[\"target\"] == 0]\n",
    "target_1 = aptos2019_train_df[aptos2019_train_df[\"target\"] == 1]\n",
    "\n",
    "test_data_propotion = 0.3\n",
    "\n",
    "n_0 = int(test_data_propotion * len(target_0))\n",
    "n_1 = int(test_data_propotion * len(target_1))\n",
    "\n",
    "sample_0 = target_0.sample(n=n_0, replace=False, random_state=0)\n",
    "sample_1 = target_1.sample(n=n_1, replace=False, random_state=1)\n",
    "\n",
    "test_split_df = pd.concat([sample_0, sample_1])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "test_split_df.to_csv('./test_split.csv')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "train_split_df = pd.merge(aptos2019_train_df, test_split_df, how=\"outer\", indicator=True)\n",
    "train_split_df = train_split_df[train_split_df[\"_merge\"] == \"left_only\"]\n",
    "train_split_df = train_split_df.drop(columns=[\"_merge\"])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_split_df.to_csv('./train_split.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Have a look at these images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "image = Image.open('/home/v1xjian2/BDL/Bayesian_DL/datasets/aptos2019-blindness-detection/train_images/0a4e1a29ffff.png')\n",
    "width, height = image.size\n",
    "print(f'Original image size: {width} x {height}')\n",
    "image = image.resize((256, 256), resample=Image.BILINEAR)\n",
    "width, height = image.size\n",
    "print(f'Image size after resize: {width} x {height}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some code for fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "from RetinopathyDataset import RetinopathyDataset\n",
    "train_data = RetinopathyDataset(csv_file='/home/v1xjian2/BDL/Bayesian_DL/blindness_detection/binarised_data.csv', image_folder='/home/v1xjian2/BDL/Bayesian_DL/datasets/aptos2019-blindness-detection/train_images')\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "train_idx = list(range(2))\n",
    "import torch\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=1, sampler=SubsetRandomSampler(train_idx))\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "for a in train_loader:\n",
    "    print()\n",
    "    print(a['image'])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_function(pred_list, output_softmax_list, target_list, model_disag_score_list, pred_entropy_list, thresholds):\n",
    "    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, average_precision_score\n",
    "    '''\n",
    "    Args:\n",
    "        pred_list: List of predicted class for test samples.\n",
    "        output_softmax_list: List of (ensembled) softmax predictions for test samples, which are averaged from n_bma_samples.\n",
    "        target_list: List of labels of test samples.\n",
    "        model_disag_score_list: List of model disagreement scores.\n",
    "        pred_entropy_list: List of entropies for predictive distributions.\n",
    "        thresholds: List of threshold we want to use in uncertainty guided prediction.\n",
    "\n",
    "    Returns:\n",
    "        All metrices we want, stored in a dirctionary.\n",
    "        The metrices are:   \n",
    "            1. Test accuracy\n",
    "            2. Precision\n",
    "            3. Recall\n",
    "            4. AU-ROC\n",
    "            5. AU-PRC\n",
    "            6. F1 score\n",
    "        These metrices are tested for (pure) prediction and for (different level of) uncertainty guided prediction.\n",
    "    '''\n",
    "    # Sort, small -> large\n",
    "    pred_entropy_sorted_indices = [pred_entropy_list.index(x) for x in sorted(pred_entropy_list)]\n",
    "    model_disag_score_indices = [model_disag_score_list.index(x) for x in sorted(model_disag_score_list)]\n",
    "\n",
    "    if 1 not in thresholds:\n",
    "        thresholds += [1]\n",
    "\n",
    "    metric_dirct = {}\n",
    "    for threshold in thresholds:\n",
    "        index =  int(np.floor(len(pred_list) * threshold))\n",
    "        curr_accuracy = accuracy_score(target_list[:index], pred_list[:index])\n",
    "        curr_precision = precision_score(target_list[:index], pred_list[:index])\n",
    "        curr_recall = recall_score(target_list[:index], pred_list[:index])\n",
    "        curr_f1 = f1_score(target_list[:index], pred_list[:index])\n",
    "        curr_auroc = roc_auc_score(target_list[:index], output_softmax_list[:index])\n",
    "        curr_auprc = average_precision_score(target_list[:index], output_softmax_list[:index])\n",
    "\n",
    "        jj = int(np.floor(threshold * 100))\n",
    "        exec(f'metric_dirct[\"accuracy_{jj}\"] = curr_accuracy')\n",
    "        exec(f'metric_dirct[\"precision_{jj}\"] = curr_precision')\n",
    "        exec(f'metric_dirct[\"recall_{jj}\"] = curr_recall')\n",
    "        exec(f'metric_dirct[\"f1_{jj}\"] = curr_f1')\n",
    "        exec(f'metric_dirct[\"auroc_{jj}\"] = curr_auroc')\n",
    "        exec(f'metric_dirct[\"auprc_{jj}\"] = curr_auprc')\n",
    "\n",
    "    return metric_dirct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3cbb5ea30b7f605f8e604c80a8a30dacfec3228ba668dffd136ee7a4411d7a8"
  },
  "kernelspec": {
   "display_name": "Python 3.9.0 (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
